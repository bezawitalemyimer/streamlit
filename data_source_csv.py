# -*- coding: utf-8 -*-
"""data_source csv.ipynb

Automatically generated by Colaboratory.

Original file is located at
   # https://colab.research.google.com/drive/1tAsdKZTzj45etXM7vOYvuZXlEJ-9OrD6
"""

#from google.colab import drive
#drive.mount("/content/drive")

"""Task 1 - User Overview analysis"""

# Commented out IPython magic to ensure Python compatibility.
#imports 
import pandas as pd      # data manipulation transformation 
import numpy as np       # numerical data analysis 
import matplotlib.pyplot as plt
import seaborn as sns    # data visualization
# %matplotlib inline 
# %config InlineBackend.figure_format = 'retina' #higher resolution
# importing the style packagegit
from matplotlib import style
# using the style for the plot
plt.style.use('ggplot')
#import sqlite3
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
import streamlit as st
import warnings
warnings.filterwarnings('ignore')
path ="/Users/bez/Desktop/streamlit/Week1_challenge_data_source.csv"
data = pd.read_csv(path)
 #data set is load and stored in pandas data frame
data.head()

data.describe

"""Explore what has been stored in it. The column names, how many data points, number of colum, data shape"""

data.info()

# column names
data.columns.tolist()

# number of data points (row ,column)
data.shape

#data types
data.dtypes

"""Handling Missing Values"""

# missing values in the dataset?
def percent_missing(dm):

    # Calculate total number of cells in dataframe
    totalCells = np.product(dm.shape)

    # Count number of missing values per column
    missingCount = dm.isnull().sum()

    # Calculate total number of missing values
    totalMissing = missingCount.sum()

    # Calculate percentage of missing values
    print("The telecom dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")

percent_missing(data)

#column(s) has missing values 
data.isna().sum().sort_values(ascending=False) #columns in descending order to see columns with highest missing values

len(data) #length of the data frame

data.isna().sum().sort_values(ascending=False)/len(data)*100 #to get % of missing values in each column descending method
#print(type(data))

"""How to fix missing values"""

# drop columns with more than 30% missing values
perc = 30.0
min_count =  int(((100-perc)/100)*data.shape[0] + 1)
mod_df = data.dropna( axis=1, thresh=min_count)

#print modified data

mod_df.isna().sum().sort_values(ascending=False)/len(data)*100 
#print(min_count)
#print(data.shape, mod_df.shape)

#print( type (mod_df))

# cleaning data frame
mod_df.dropna(subset = ["Bearer Id", "MSISDN/Number"], inplace=True)
cols=['Avg RTT DL (ms)','Avg RTT UL (ms)','Last Location Name','10 Kbps < UL TP < 50 Kbps (%)','Nb of sec with Vol UL < 1250B','50 Kbps < UL TP < 300 Kbps (%)','10 Kbps < UL TP < 50 Kbps (%)','UL TP < 10 Kbps (%)',
      'UL TP > 300 Kbps (%)','Nb of sec with Vol DL < 6250B','DL TP > 1 Mbps (%)','250 Kbps < DL TP < 1 Mbps (%)','50 Kbps < DL TP < 250 Kbps (%)','DL TP < 50 Kbps (%)']
for col in cols:
  mod_df[col]=mod_df[col].fillna(method='ffill')

mod_df.isna().sum().sort_values(ascending=False)/len(data)*100

#data.isna().sum().sort_values(ascending=False)/len(data)*100
#print (type(data))



"""Transforming Data
Scaling and Normalization
"""

from sklearn.preprocessing import MinMaxScaler

minmax_scaler = MinMaxScaler()

# generate 1000 data points randomly drawn from an exponential distribution
original_data = pd.DataFrame(np.random.exponential(200, size=2000))

original_data.sample(5)

original_data[0].min(), original_data[0].max()

count, bins, ignored = plt.hist(original_data, 14)
plt.show()

# mix-max scale the data between 0 and 1
def scaler(df):
    scaled_data = minmax_scaler.fit_transform(df)

    # plot both together to compare
    fig, ax = plt.subplots(1,2, figsize=(10, 6))
    sns.histplot(original_data, ax=ax[0])
    ax[0].set_title("Original Data")
    sns.histplot(scaled_data, ax=ax[1])
    ax[1].set_title("Scaled data")
    
scaler(original_data)

from sklearn.preprocessing import Normalizer

def normalizer(df):
    norm = Normalizer()
    # normalize the exponential data with boxcox
    normalized_data = norm.fit_transform(df)

    # plot both together to compare
    fig, ax=plt.subplots(1,2, figsize=(10, 6))
    sns.histplot(df, ax=ax[0])
    ax[0].set_title("Original Data")
    sns.histplot(normalized_data[0], ax=ax[1])
    ax[1].set_title("Normalized data")

normalizer(original_data)

data.info()

"""For the actual telecom dataset, you‘re expected to conduct a full User Overview analysis &
the following sub-tasks are your guidance:   
● Start by identifying the top 10 handsets used by the customers.   
● Then, identify the top 3 handset manufacturers  
● Next, identify the top 5 handsets per top 3 handset manufacturer   
● Make a short interpretation and recommendation to marketing teams

"""

#Start by identifying the top 10 handsets used by the customers.
data["Handset Type"].value_counts().head(10)

#identify the top 3 handset manufacturers
data["Handset Manufacturer"].value_counts().head(3)

#identify the top 5 handsets per top 3 handset manufacturer
apple_handsets = data[data["Handset Manufacturer"] == "Apple"]
apple_handsets["Handset Type"].value_counts().head(5)

apple_handsets = data[data["Handset Manufacturer"] == "Samsung"]
apple_handsets["Handset Type"].value_counts().head(5)

apple_handsets = data[data["Handset Manufacturer"] == "Huawei"]
apple_handsets["Handset Type"].value_counts().head(5)

#● Make a short interpretation and recommendation to marketing teams

"""*  The marketing team needs to focus on users with handsets from these top three manufacturers: Apple, Samsung and Huawei."""



"""**Analysis**

Task 1.1 - Your employer wants to have an overview of the users’ behavior on those applications.     
● Aggregate per user the following information in the column     
○ number of xDR sessions     
○ Session duration      
○ the total download (DL) and upload (UL) data     
○ the total data volume (in Bytes) during this session for each application
"""

aggrigate = {"Bearer Id": 'count', 'Dur. (ms)':'sum', 'Total UL (Bytes)': 'sum', 'Total DL (Bytes)': 'sum'}
aggrigation_res =data.groupby('MSISDN/Number').agg(aggrigate)
aggrigation_res.describe()

sessions = data[["MSISDN/Number","Bearer Id"]]
session_count = sessions.groupby(["MSISDN/Number"]).count()
session_count

session_count.describe()

# data usage 
data_usage = data[["MSISDN/Number","Dur. (ms).1", "Total DL (Bytes)", "Total UL (Bytes)"]]
data_usage.groupby(["MSISDN/Number"]).agg('sum')

"""**user Behaviour**"""

# user
aggrigate = {
    'Total Social Media':'sum',
    'Total Google': 'sum', 
    'Total Youtube': 'sum', 
    'Total Netflix':'sum',
    'Total Gaming':'sum',
    'Total Other':'sum',
    'Total Email': 'sum',
    'Total':'sum'
    }
data = data.copy()

data["Total Google"] = data["Google DL (Bytes)"] + data["Google UL (Bytes)"]
data["Total Youtube"] = data["Youtube DL (Bytes)"] + data["Youtube UL (Bytes)"]
data["Total Netflix"] = data["Netflix DL (Bytes)"] + data["Netflix UL (Bytes)"]
data["Total Email"] = data["Email DL (Bytes)"] + data["Email UL (Bytes)"]
data["Total Gaming"] = data["Gaming DL (Bytes)"] + data["Gaming UL (Bytes)"]
data["Total Social Media"] = data["Social Media DL (Bytes)"] + data["Social Media UL (Bytes)"]
data["Total Other"] = data["Other DL (Bytes)"] + data["Other UL (Bytes)"]
data["Total "] = data["Total DL (Bytes)"] + data["Total UL (Bytes)"]

data_usage = data[["Bearer Id","MSISDN/Number","Total Google", "Total Youtube", "Total Netflix", "Total Email", "Total Gaming", 
                 "Total Social Media", "Total Other"]]
data.groupby(["MSISDN/Number"]).agg('sum')
#app_data.info()



"""Task 1.2 - Conduct an exploratory data analysis on those data & communicate useful insights. Ensure that you identify and treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column."""

#checking for null values
data[["Dur. (ms).1","Total DL (Bytes)","Total UL (Bytes)", "Total Social Media", "Total Google", "Total Email", "Total Youtube",
      "Total Netflix", "Total Gaming"]].isna().sum().head(60)

#filling the null values with the mean of the column
data["Total DL (Bytes)"].fillna(data["Total DL (Bytes)"].mean(), inplace = True)
data["Total UL (Bytes)"].fillna(data["Total UL (Bytes)"].mean(), inplace = True)
data["Dur. (ms).1"].fillna(data["Dur. (ms).1"].mean(), inplace = True)

data[["Dur. (ms).1", "Total DL (Bytes)","Total UL (Bytes)", "Total Social Media", "Total Google", "Total Email", "Total Youtube",
      "Total Netflix", "Total Gaming"]].describe()

#checking for skewness
data.skew()

#looking for the outliers
data.boxplot(column=['Dur. (ms).1', "Total DL (Bytes)","Total UL (Bytes)"])

"""**Graphical** **Analysis**

plotting variable distributions
"""

#plotting Histogram for Duration
data["Dur. (ms).1"].hist(bins = 50)
plt.title("Duration Histogram")

#plotting Histogram for Total Download 

data["Total DL (Bytes)"].hist(bins = 50)
plt.title("Total Download Histogram")

#plotting Histogram for Total Upload 
data["Total UL (Bytes)"].hist(bins = 50)
plt.title("Total Upload Histogram")

#plotting Histogram for Total Social Media Data
data["Total Social Media"].hist(bins = 50)
plt.title("Total Social Media Data Histogram")

# plotting Histogram for Total Google Data
data["Total Google"].hist(bins = 50)
plt.title("Total Google Data Histogram")

#plotting Histogram  for Total Email Data
data["Total Email"].hist(bins = 50)
plt.title("Total Email Data Histogram")

#plotting Histogram for Total Youtube Data 
data["Total Youtube"].hist(bins = 50)
plt.title("Total Youtube Data Histogram")

#plotting Histogram for Total Netflix
data["Total Netflix"].hist(bins = 50)
plt.title("Total Netflix Data Histogram")

#plotting Histogram for Total Gaming Data
data["Total Gaming"].hist(bins = 50)
plt.title("Total Gaming Data Histogram")

#plotting comparisons
total_usage = data_usage[["Total Google", "Total Youtube", "Total Netflix", "Total Email", "Total Gaming", "Total Social Media", "Total Other"]]
ind = np.arange(7)
width =0.8

plt.xticks(ind , ("Google", "Youtube", "Netflix", "Email", "Gaming", "Social", "Other"))
plt.bar(ind, total_usage.sum(), width, label = "Total Usage")
plt.title("App Data usage")

"""**Bivariate** **Analysis**"""

data.plot.line()

"""**Variable** **transformations**   
segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class
"""

#group users into top 5 deciles 
users = data.groupby(["MSISDN/Number"]).agg({'Bearer Id':'count', 'Dur. (ms).1':'sum', 'Total UL (Bytes)':'sum', 'Total DL (Bytes)':'sum'}).sort_values(by="Dur. (ms).1", ascending = False)
first_decile = users.iloc[0:10,:]
second_decile = users.iloc[10:20,:]
third_decile = users.iloc[20:30,:]
fourth_decile = users.iloc[30:40,:]
fifth_decile = users.iloc[40:50,:]

#print total data of each decile group
print("Total data of the first decile is: ",first_decile["Total DL (Bytes)"].sum() + first_decile["Total UL (Bytes)"].sum())
print("Total data of the second decile is: ",second_decile["Total DL (Bytes)"].sum() + second_decile["Total UL (Bytes)"].sum())
print("Total data of the third decile is: ",third_decile["Total DL (Bytes)"].sum() + third_decile["Total UL (Bytes)"].sum())
print("Total data of the fourth decile is: ",fourth_decile["Total DL (Bytes)"].sum() + fourth_decile["Total UL (Bytes)"].sum())
print("Total data of the fifth decile is: ",fifth_decile["Total DL (Bytes)"].sum() + fifth_decile["Total UL (Bytes)"].sum())

"""**Correlation Analysis** – compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email,data, Youtube data, Netflix data, Gaming data, Other data"""

#correlation/Relationship of the variables /Applications)
correlation = total_usage.corr()
sns.heatmap(correlation, annot=True)
plt.show()

"""**Dimensionality** **Reduction**   
 perform a principal component analysis to reduce the dimensions of your data and provide a useful interpretation of the results (Provide your interpretation in four (4) bullet points-maximum)
"""

#use PCA to get 4 principal components

pca = PCA(n_components=4)
principal_components = pca.fit_transform(total_usage)
principal_Df = pd.DataFrame(data = principal_components
             , columns = ['pc_1', 'pc_2', 'pc_3', 'pc_4'])
principal_Df

"""Task 2 - **User** **Engagement** **analysis**.    
engagement metrics:

● sessions frequency.       
● the duration of the session.      
● the sessions total traffic (download and upload (bytes))                        
Task 2.1 - Based on the above:

Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric 
Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. 
"""

#adding total traffic to the data
data["Total Traffic"] = data["Total UL (Bytes)"] + data["Total DL (Bytes)"]
engagement = data.groupby('MSISDN/Number').agg({'Bearer Id':'count', 'Dur. (ms).1':'sum', 'Total Traffic':'sum'})


print("Top ten frequent users: \n", engagement.sort_values(by="Bearer Id", ascending=False).head(10))
print("\n\nTop ten users with the longest sessions: \n", engagement.sort_values(by="Dur. (ms).1", ascending=False).head(10))
print("\n\nTop ten users with the most data usage: \n", engagement.sort_values(by="Total Traffic", ascending=False).head(10))

engagement.describe()

"""**Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement**"""

# normalizing the engagement dataframe
norm_engagement = (engagement - engagement.mean())/engagement.std()
norm_engagement.describe()

#checking the normalized variables
norm_engagement["Bearer Id"].hist(bins=50)

norm_engagement["Dur. (ms).1"].hist(bins=50)

norm_engagement["Total Traffic"].hist(bins=50)

#perform kmeans clustering with k=3
kmeans_3k = KMeans(n_clusters = 3)
kmeans_3k.fit(norm_engagement)

#assign cluster to each user 
engagement_centroids = kmeans_3k.cluster_centers_
cluster_3k = kmeans_3k.predict(norm_engagement)
norm_engagement['cluster_3k'] = cluster_3k

#plotting clusters
sns.scatterplot(x=norm_engagement.iloc[:,0], y=norm_engagement.iloc[:,1], hue=norm_engagement["cluster_3k"], legend="full")

"""Compute the minimum, maximum, average & total non- normalized metrics for each cluster. Interpret your results visually with accompanying text."""

engagement["cluster_3k"]=cluster_3k
sns.scatterplot(x=engagement["Dur. (ms).1"], y=engagement["Total Traffic"], hue=engagement["cluster_3k"], legend="full")

#number of users in each cluster
print("C.   Total Users")
engagement["cluster_3k"].value_counts()



engagement[engagement.cluster_3k == 0].describe()

engagement[engagement.cluster_3k == 1].describe()

engagement[engagement.cluster_3k == 2].describe()

"""Aggregate user total traffic per application and derive the top 10 most engaged users per application - (jupyter notebook + slide)
Plot the top 3 most used applications. 
"""

app_total = data.groupby('MSISDN/Number').agg({ 'Total Traffic':'sum',"Total Google":'sum', "Total Youtube":'sum', "Total Netflix":'sum', "Total Email":'sum', "Total Gaming":'sum', "Total Social Media":'sum', "Total Other":'sum'})


for coll in app_total:
    print("\n\nTop ten ",coll," users: \n", app_total.sort_values(by=coll, ascending=False)[coll].head(10))

#print top 3 used apps
top_used = pd.DataFrame(columns=["Name", "Total Data"])
for col in app_total:
    top_used.loc[len(top_used)] = [col, app_total[col].sum()]

top_three = top_used.sort_values(by = "Total Data", ascending = False)[1:4]
print("Top three used applications: ")
top_three

#plot the top 3 apps
ind = np.arange(3)
width =0.8

plt.xticks(ind , ("Gaming", "Other", "YouTube"))
plt.bar(ind, top_three["Total Data"], width, label = "Total Usage")
plt.title("Top 3 most used apps")

"""Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics:                                             
What is the optimized value of k?      
Interpret your findings. 
"""

#calculating inertia for k values 1 -19
inertia = []
for cluster in range(1,20):
    km = KMeans(n_jobs = -1, n_clusters = cluster)
    km.fit(norm_engagement)
    inertia.append(km.inertia_)

#plotting k Vs inertia 
frame = pd.DataFrame({'k':range(1,20), 'inertia':inertia})

plt.figure(figsize=(12,6))
plt.plot(frame['k'], frame['inertia'], marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

#taking k = 4 and clustering
kmeans_4k = KMeans(n_clusters=4)
kmeans_4k.fit(norm_engagement)

sns.scatterplot(x=norm_engagement.iloc[:,0], y=norm_engagement.iloc[:,1], hue=kmeans_4k.labels_, legend="full")

"""Task 3. 1 - Aggregate, per customer, the following information (treat missing & outliers by replacing by the mean or the mode of the corresponding variable):         
● Average TCP retransmission   
● Average RTT                 
● Handset type
"""

#adding total network parameters
data["Total RTT"] = data["Avg RTT DL (ms)"] + data["Avg RTT UL (ms)"]
data["Total TP"] = data["Avg Bearer TP DL (kbps)"] + data["Avg Bearer TP UL (kbps)"]
data["Total TCP"] = data["TCP DL Retrans. Vol (Bytes)"] + data["TCP UL Retrans. Vol (Bytes)"]
experience = data.groupby('MSISDN/Number').agg({'Total TCP':'sum', 'Total RTT':'sum', 'Total TP':'sum', 'Handset Type':'count'})#

experience

"""Task 3.2 - Compute & list 10 of the top, bottom and most frequent:    
a. TCP values in the dataset.    
b. RTT values in the dataset.    
c. Throughput values in the dataset.
"""

print("\n\nTop ten TCP values: \n", experience.sort_values(by="Total TCP", ascending=False).head(10))
print("\n\nTop ten RTT values: \n", experience.sort_values(by="Total RTT", ascending=False).head(10))
print("\n\nTop ten Throughput values: \n", experience.sort_values(by="Total TP", ascending=False).head(10))

print("\n\nBottom ten TCP values: \n", experience.sort_values(by="Total TCP", ascending=True).head(10))
print("\n\nBottom ten RTT values: \n", experience.sort_values(by="Total RTT", ascending=True).head(10))
print("\n\nBottom ten Throughput values: \n", experience.sort_values(by="Total TP", ascending=True).head(10))

top10_tcp = experience.groupby('Total TCP').size().sort_values(ascending = False).head(10)
top10_tcp

top10_rtt = experience.groupby('Total RTT').size().sort_values(ascending = False).head(10)
top10_rtt

top10_tp = experience.groupby('Total TP').size().sort_values(ascending = False).head(10)
top10_tp


print("\n\nTen most frequent TCP values: \n", top10_tcp)
print("\n\nTen most frequent RTT values: \n", top10_rtt)
print("\n\nTen most frequent Throughput values: \n", top10_tp)



"""Task 3.3 - Compute & report 

The distribution of the average throughput per handset type and provide interpretation for your findings.
The average TCP retransmission view per handset type and provide interpretation for your findings.
"""

handset = data.groupby('Handset Type').agg({'Total TCP':'sum', 'Total RTT':'sum', 'Total TP':'sum'})

print("\n\nThroughput\n",handset.sort_values(by="Total TP", ascending = False).head(10))
print("\n\nTCP Retransmission\n",handset.sort_values(by="Total TCP", ascending = False).head(10))

"""Task 3.4 - Using the experience metrics above, perform a k-means clustering (where k = 3) to segment users into groups of experiences and provide a brief description of each cluster"""

#normalizing the experience dataframe
norm_experience = (experience - experience.mean())/experience.std()
norm_experience

#performing kmeans with k=3
kmeans_exp = KMeans(n_clusters=3)
kmeans_exp.fit(norm_experience)
cluster_exp = kmeans_exp.predict(norm_experience)
experience_centroids = kmeans_exp.cluster_centers_

#assign the clusters
norm_experience['cluster_exp'] = cluster_exp
norm_experience['cluster_exp'].value_counts()

#plot the clusters
sns.scatterplot(x=norm_experience.iloc[:,2], y=norm_experience.iloc[:,0], hue=kmeans_exp.labels_, legend="full")

"""Task 4

Task 4. 1 - Write a python program to assign:

a. engagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & the less engaged cluster (use the first clustering for this) - (jupyter notebook ) from scipy.spatial import distance

b. experience score to each user. Consider the experience score as the Euclidean distance between the user data point & the worst experience’s cluster.
"""

#compute each cluster mean to find the worst experienced cluster
print(norm_experience[norm_experience["cluster_exp"]==0].mean())
print(norm_experience[norm_experience["cluster_exp"]==1].mean())
print(norm_experience[norm_experience["cluster_exp"]==2].mean())

#compute each cluster mean to find the least engaged group
print(norm_engagement[norm_engagement["cluster_3k"]==0].mean())
print(norm_engagement[norm_engagement["cluster_3k"]==1].mean())
print(norm_engagement[norm_engagement["cluster_3k"]==2].mean())

#Calculating Euclidean distance to get engagement and experience scores
satisfaction = pd.DataFrame()
satisfaction['eng_score'] = ((norm_engagement[["Bearer Id","Dur. (ms).1","Total Traffic"]] - np.array(engagement_centroids[1])).pow(2)).sum(1).pow(0.5)
satisfaction['exp_score'] = ((norm_experience[["Total TCP","Total RTT","Total TP","Handset Type"]] - np.array(experience_centroids[0])).pow(2)).sum(1).pow(0.5)

satisfaction

"""Task 4.2 - Consider the average of both engagement & experience scores as the satisfaction score & report the top 10 satisfied customer"""

#Taking the average of engagement and experience as a satisfaction score
satisfaction["satisfaction_score"] = (satisfaction['eng_score'] + satisfaction['exp_score'])/2
satisfaction.sort_values(by="satisfaction_score", ascending = False).head(10)

"""Task 4.3 - Run a regression model of your choice to predict the satisfaction score of a customer."""

#Using linear regression model to predict user satisfaction
model = LinearRegression()
inputs = satisfaction[["eng_score","exp_score"]]
target = satisfaction["satisfaction_score"]
model.fit(inputs,target)

#intercept and coefficients of the model z=ax + by +c
print('Intercept: ', model.intercept_)
print('Coefficients: ', model.coef_)
print ('Predicted Satisfaction Score: ', model.predict([[10,8]]))

"""Task 4.4 - Run a k-means (k=2) on the engagement & the experience score"""

#running kmeans with k=2 to group users
kmeans_score = KMeans(n_clusters=2)
kmeans_score.fit(satisfaction[["eng_score","exp_score"]])
score_cluster = kmeans_score.predict(satisfaction[["eng_score","exp_score"]])
score_centroids = kmeans_score.cluster_centers_

#Assigning cluster to each user
satisfaction['score_cluster'] = score_cluster
satisfaction

"""Task 4.5 - Aggregate the average satisfaction & experience score per cluster."""

#Average satisfaction and experience score of cluster 0
print("Cluster 0 average experience score",satisfaction[satisfaction['score_cluster']==0]["exp_score"].mean())
print("Cluster 0 average satisfaction score",satisfaction[satisfaction['score_cluster']==0]["satisfaction_score"].mean())

#Average satisfaction and experience score of cluster 1
print("Cluster 1 average experience score",satisfaction[satisfaction['score_cluster']==0]["exp_score"].mean())
print("Cluster 1 average satisfaction score",satisfaction[satisfaction['score_cluster']==0]["satisfaction_score"].mean())

sns.scatterplot(x=satisfaction.iloc[:,0], y=satisfaction.iloc[:,1], hue=kmeans_score.labels_, legend="full")

"""Task 4.6 - Export your final table containing all user id + engagement, experience & satisfaction scores in your local MySQL database. Report a screenshot of a select on the exported table."""

#subsetting the dataframe to export
export = satisfaction[["eng_score", "exp_score", "satisfaction_score"]]

#connecting to local dtabase and writing the dataframe
connection = sqlite3.connect('tellco.db')
export.to_sql(name = 'satisfaction', con = connection, if_exists = 'replace')
connection.close()

#Retrieving the dataframe from local database
connection = sqlite3.connect('tellco.db')
query_output = pd.read_sql('select "MSISDN/Number", '+
        '"satisfaction_score" from satisfaction', connection)
print(query_output)
connection.close()